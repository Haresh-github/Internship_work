{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import pandas as pd\n",
    "import json\n",
    "from spacy import displacy\n",
    "from spacy.language import Language\n",
    " \n",
    "# INTENTS and ENTITIES from your dataset\n",
    "INTENTS = [\n",
    "    \"Total (Manpower_Manhour) are present in (discipline)\",\n",
    "    \"Total (Manpower_Manhour) are present in (discipline) for (Month)\",\n",
    "    \"Total (Manpower_Manhour) are present in (discipline) from (Month) till (Month)\"\n",
    "    \"Manhours Plan vs Actuals for a (month) and (discipline)\"\n",
    "    \"Difference in Manmonths  Plan vs Actuals for a particular month and discipline\"\n",
    "    \"Count of manpower for a particular discipline for a month\"\n",
    "\n",
    "]\n",
    "\n",
    "ENTITIES = {\n",
    "    \"Discipline\": [\"HSE\", \"CIVIL\", \"PROCESS\", \"ALL\",\"CATHODIC PROTECTION\",\"COMMISSIONING\",\"DIGITAL\",\"DOCUMENT CONTROL\",\"HVAC\", \"INSTRUMENTATION AND ENVIRONMENT\",\"MANUFACTURING\",\"MECHANICAL\",\"METALLURGY\",\"PROCESS\",\"PRODUCTION\",\"PROTECTION\",\"QUALITY\",\"SUBCONTRACT ELECTRICAL\",\"TECHNICAL MANAGEMENT\",\"STATIC\",\"PROJECT MANAGEMENT\",\"PROJECT CONTROL\"],\n",
    "    \"Month\": [\"Nov-2024\", \"Apr-2024\", \"JAN-2023\",\"SEP-2023\", \"JUL-2023\",\"FEB-2023\",\"any date in form of mm-yy or mmm-yyyy\", \"current month\", \"CURRENT MONTH\",\"today\"],\n",
    "    \"Manhour_Manpower_type\": [\"manpower\", \"manhour\", \"man hour\", \"man power\",\"manmonths\", \"man months\", \"manmonnth\", \"man month\", \"Manhours Plan vs Actuals\", \"Manhour Planned VS Actual\"]\n",
    "}\n",
    " \n",
    "# === INTENT CLASSIFIER COMPONENT ===\n",
    "@Language.factory(\"intent_classifier\")\n",
    "class IntentClassifier:\n",
    "    def __init__(self, nlp, name, model_name=\"roberta-base\"):\n",
    "        print(\"Loading tokenizer...\")\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        print(\"Loading model...\")\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=len(INTENTS))\n",
    "        print(\"Model loaded successfully!\")\n",
    " \n",
    "    def classify_intent(self, user_input):\n",
    "        print(f\"Classifying intent for input: {user_input}\")\n",
    "        inputs = self.tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_id = torch.argmax(logits).item()\n",
    "        return INTENTS[predicted_class_id]\n",
    " \n",
    "    def __call__(self, doc):\n",
    "        intent = self.classify_intent(doc.text)\n",
    "        doc._.intent = intent\n",
    "        return doc\n",
    " \n",
    "# === ENTITY MATCHER FUNCTION ===\n",
    "def match_entities(text):\n",
    "    matched_entities = []\n",
    "    for entity_type, values in ENTITIES.items():\n",
    "        for value in values:\n",
    "            if value.lower() in text.lower():  # Case insensitive matching\n",
    "                matched_entities.append({\"entity_name\": entity_type, \"value\": value})\n",
    "    return matched_entities\n",
    " \n",
    "# === CUSTOM COMPONENT TO EXTRACT ENTITIES ===\n",
    "@Language.component(\"entity_extractor\")\n",
    "def extract_entities(doc):\n",
    "    matched_entities = match_entities(doc.text)  # Match entities based on user input text\n",
    "    doc._.entities = matched_entities\n",
    "    return doc\n",
    " \n",
    "# === DATASET MANAGER CLASS ===\n",
    "class DatasetManager:\n",
    "    def __init__(self):\n",
    "        self.dataset = None\n",
    " \n",
    "    def load_dataset(self):\n",
    "        # Ask for the file path directly via input() instead of opening a dialog\n",
    "        file_path = input(\"Enter the full path to the dataset (CSV or Excel): \")\n",
    " \n",
    "        if file_path.endswith('.csv'):\n",
    "            self.dataset = pd.read_csv(file_path)\n",
    "        elif file_path.endswith('.xlsx'):\n",
    "            self.dataset = pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format\")\n",
    "        print(f\"Dataset loaded from: {file_path}\")\n",
    " \n",
    "    def get_dataset(self):\n",
    "        return self.dataset\n",
    " \n",
    "# === ANNOTATION FUNCTION FOR VISUALIZING ENTITIES ===\n",
    "def annotate_user_input(doc):\n",
    "    \"\"\"\n",
    "    Annotates and highlights entities in the user input using spaCy's displacy.\n",
    "    \"\"\"\n",
    "    # Prepare the visualization data\n",
    "    ents = []\n",
    "    for entity in doc._.entities:\n",
    "        # Find where the entity is in the doc\n",
    "        start = doc.text.find(entity['value'])\n",
    "        end = start + len(entity['value'])\n",
    "        ents.append({\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"label\": entity['entity_name']\n",
    "        })\n",
    "    # Create a custom displacy-friendly structure\n",
    "    displacy_data = {\n",
    "        \"text\": doc.text,\n",
    "        \"ents\": ents,\n",
    "        \"title\": None\n",
    "    }\n",
    " \n",
    "    # Visualize using displacy (spaCy's visualization tool)\n",
    "    displacy.render(displacy_data, style=\"ent\", manual=True)\n",
    " \n",
    "# === MAIN FUNCTION TO PROCESS QUERY ===\n",
    "def process_query(nlp, user_input):\n",
    "    # Process the query with spaCy's pipeline\n",
    "    print(f\"Processing query: {user_input}\")\n",
    "    doc = nlp(user_input)\n",
    " \n",
    "    # Get intent and entities\n",
    "    intent = doc._.intent\n",
    "    entities = doc._.entities\n",
    " \n",
    "    # Annotate the user input to highlight entities\n",
    "    annotate_user_input(doc)\n",
    " \n",
    "    # Return the response with intent and entities\n",
    "    return {\n",
    "        \"User Query\": user_input,\n",
    "        \"Intent\": intent,\n",
    "        \"Entities\": entities\n",
    "    }\n",
    " \n",
    "# === MAIN WORKFLOW SETUP ===\n",
    "def main():\n",
    "    # Load spaCy and add components\n",
    "    print(\"Loading spaCy pipeline...\")\n",
    "    nlp = spacy.blank(\"en\")\n",
    " \n",
    "    # Add custom components to the pipeline\n",
    "    print(\"Adding custom components to pipeline.\")\n",
    "    nlp.add_pipe(\"intent_classifier\", last=True)  # Registering the intent classifier\n",
    "    nlp.add_pipe(\"entity_extractor\", last=True)  # Adding entity extractor after intent classifier\n",
    "    print(\"Components added to pipeline.\")\n",
    " \n",
    "    # Define custom attributes\n",
    "    spacy.tokens.Doc.set_extension(\"intent\", default=None)\n",
    "    spacy.tokens.Doc.set_extension(\"entities\", default=[])\n",
    " \n",
    "    # Load dataset (using DatasetManager)\n",
    "    dataset_manager = DatasetManager()\n",
    "    dataset_manager.load_dataset()\n",
    "    print(\"Dataset loaded.\")\n",
    " \n",
    "    # Receive user input\n",
    "    user_query = input(\"Enter your query: \")\n",
    "    print(f\"User query received: {user_query}\")\n",
    " \n",
    "    # Process the query and print the result\n",
    "    response = process_query(nlp, user_query)\n",
    "    print(json.dumps(response, indent=4))\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "import json\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "from spacy.language import Language\n",
    "\n",
    "# INTENTS and ENTITIES from your dataset\n",
    "\n",
    "INTENTS = [\n",
    "\n",
    "    \"Total (Manpower_Manhour) are present in (discipline)\",\n",
    "\n",
    "    \"Total (Manpower_Manhour) are present in (discipline) for (Month)\",\n",
    "\n",
    "    \"Total (Manpower_Manhour) are present in (discipline) from (Month) till (Month)\",\n",
    "\n",
    "    \"Manhours Plan vs Actuals for a (month) and (discipline)\",\n",
    "\n",
    "    \"Difference in Manmonths Plan vs Actuals for a particular month and discipline\",\n",
    "\n",
    "    \"Count of manpower for a particular discipline for a month\"\n",
    "\n",
    "]\n",
    "\n",
    "ENTITIES = {\n",
    "\n",
    "    \"Discipline\": [\"All\", \"Cathodic Protection\", \"Civil\", \"Commissioning\", \"Digital\", \"Document Control\", \"HVAC\", \"HSE\",\n",
    "                   \"Instrumentation and Environment\", \"Manufacturing\", \"Mechanical\", \"Metallurgy\", \"Process\", \"Production\",\n",
    "                   \"Project Control\", \"Project Management\", \"Protection\", \"Quality\", \"Static\", \"Subcontract Electrical\", \"Technical Management\"],\n",
    "\n",
    "    \"Month\": [\"Nov-2024\", \"Apr-2024\", \"JAN-2023\", \"SEP-2023\", \"JUL-2023\", \n",
    "\n",
    "              \"FEB-2023\", \"any date in form of mm-yy or mmm-yyyy\", \"current month\", \n",
    "\n",
    "              \"CURRENT MONTH\", \"today\"],\n",
    "\n",
    "    \"Manhour_Manpower_type\": [\"manpower\", \"manhour\", \"man hour\", \"man power\", \n",
    "\n",
    "                              \"manmonths\", \"man months\", \"manmonnth\", \"man month\", \n",
    "\n",
    "                              \"Manhours Plan vs Actuals\", \"Manhour Planned VS Actual\"]\n",
    "\n",
    "}\n",
    "\n",
    "# === INTENT CLASSIFIER COMPONENT ===\n",
    "\n",
    "@Language.factory(\"intent_classifier\")\n",
    "\n",
    "class IntentClassifier:\n",
    "\n",
    "    def __init__(self, nlp, name, model_name=\"roberta-base\"):\n",
    "\n",
    "        print(\"Loading tokenizer...\")\n",
    "\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        print(\"Loading model...\")\n",
    "\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=len(INTENTS))\n",
    "\n",
    "        print(\"Model loaded successfully!\")\n",
    "\n",
    "    def classify_intent(self, user_input):\n",
    "\n",
    "        print(f\"Classifying intent for input: {user_input}\")\n",
    "\n",
    "        inputs = self.tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        outputs = self.model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        predicted_class_id = torch.argmax(logits).item()\n",
    "\n",
    "        return INTENTS[predicted_class_id]\n",
    "\n",
    "    def __call__(self, doc):\n",
    "\n",
    "        intent = self.classify_intent(doc.text)\n",
    "\n",
    "        doc._.intent = intent\n",
    "\n",
    "        return doc\n",
    "\n",
    "# === ENTITY MATCHER FUNCTION ===\n",
    "\n",
    "def match_entities(text):\n",
    "\n",
    "    matched_entities = []\n",
    "\n",
    "    for entity_type, values in ENTITIES.items():\n",
    "\n",
    "        for value in values:\n",
    "\n",
    "            if value.lower() in text.lower():  # Case insensitive matching\n",
    "\n",
    "                matched_entities.append({\"entity_name\": entity_type, \"value\": value})\n",
    "\n",
    "    return matched_entities\n",
    "\n",
    "# === CUSTOM COMPONENT TO EXTRACT ENTITIES ===\n",
    "\n",
    "@Language.component(\"entity_extractor\")\n",
    "\n",
    "def extract_entities(doc):\n",
    "\n",
    "    matched_entities = match_entities(doc.text)  # Match entities based on user input text\n",
    "\n",
    "    doc._.entities = matched_entities\n",
    "\n",
    "    return doc\n",
    "\n",
    "# === ANNOTATION FUNCTION FOR VISUALIZING ENTITIES ===\n",
    "\n",
    "def annotate_user_input(doc):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Annotates and highlights entities in the user input using spaCy's displacy.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare the visualization data\n",
    "\n",
    "    ents = []\n",
    "\n",
    "    for entity in doc._.entities:\n",
    "\n",
    "        # Find where the entity is in the doc\n",
    "\n",
    "        start = doc.text.find(entity['value'])\n",
    "\n",
    "        end = start + len(entity['value'])\n",
    "\n",
    "        ents.append({\n",
    "\n",
    "            \"start\": start,\n",
    "\n",
    "            \"end\": end,\n",
    "\n",
    "            \"label\": entity['entity_name']\n",
    "\n",
    "        })\n",
    "\n",
    "    # Create a custom displacy-friendly structure\n",
    "\n",
    "    displacy_data = {\n",
    "\n",
    "        \"text\": doc.text,\n",
    "\n",
    "        \"ents\": ents,\n",
    "\n",
    "        \"title\": None\n",
    "\n",
    "    }\n",
    "\n",
    "    # Visualize using displacy (spaCy's visualization tool)\n",
    "\n",
    "    displacy.render(displacy_data, style=\"ent\", manual=True)\n",
    "\n",
    "# === MAIN FUNCTION TO PROCESS QUERY ===\n",
    "\n",
    "def process_query(nlp, user_input):\n",
    "\n",
    "    # Process the query with spaCy's pipeline\n",
    "\n",
    "    print(f\"Processing query: {user_input}\")\n",
    "\n",
    "    doc = nlp(user_input)\n",
    "\n",
    "    # Get intent and entities\n",
    "\n",
    "    intent = doc._.intent\n",
    "\n",
    "    entities = doc._.entities\n",
    "\n",
    "    # Annotate the user input to highlight entities\n",
    "\n",
    "    annotate_user_input(doc)\n",
    "\n",
    "    # Return the response with intent and entities\n",
    "\n",
    "    return {\n",
    "\n",
    "        \"User Query\": user_input,\n",
    "\n",
    "        \"Intent\": intent,\n",
    "\n",
    "        \"Entities\": entities\n",
    "\n",
    "    }\n",
    "\n",
    "# === MAIN WORKFLOW SETUP ===\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Load spaCy and add components\n",
    "\n",
    "    print(\"Loading spaCy pipeline...\")\n",
    "\n",
    "    nlp = spacy.blank(\"en\")\n",
    "\n",
    "    # Add custom components to the pipeline\n",
    "\n",
    "    print(\"Adding custom components to pipeline.\")\n",
    "\n",
    "    nlp.add_pipe(\"intent_classifier\", last=True)  # Registering the intent classifier\n",
    "\n",
    "    nlp.add_pipe(\"entity_extractor\", last=True)  # Adding entity extractor after intent classifier\n",
    "\n",
    "    print(\"Components added to pipeline.\")\n",
    "\n",
    "    # Define custom attributes\n",
    "\n",
    "    spacy.tokens.Doc.set_extension(\"intent\", default=None)\n",
    "\n",
    "    spacy.tokens.Doc.set_extension(\"entities\", default=[])\n",
    "\n",
    "    # Loop for dynamic user input\n",
    "\n",
    "    while True:\n",
    "\n",
    "        user_query = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "\n",
    "        if user_query.lower() == 'exit':\n",
    "\n",
    "            break\n",
    "\n",
    "        # Process the query and print the result\n",
    "\n",
    "        response = process_query(nlp, user_query)\n",
    "\n",
    "        print(json.dumps(response, indent=4))\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import yaml\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "import json\n",
    "from spacy import displacy\n",
    "from spacy.language import Language\n",
    " \n",
    "# Load the configuration from the YAML file\n",
    "with open('config.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    " \n",
    "# INTENTS and ENTITIES from your dataset\n",
    "INTENTS = [\n",
    "    \"Total (Manpower_Manhour) are present in (discipline)\",\n",
    "    \"Total (Manpower_Manhour) are present in (discipline) for (Month)\",\n",
    "    \"Total (Manpower_Manhour) are present in (discipline) from (Month) till (Month)\",\n",
    "    \"Manhours Plan vs Actuals for a (month) and (discipline)\",\n",
    "    \"Difference in Manmonths Plan vs Actuals for a particular month and discipline\",\n",
    "    \"Count of manpower for a particular discipline for a month\"\n",
    "]\n",
    " \n",
    "ENTITIES = {\n",
    "    \"Discipline\": [\"HSE\", \"CIVIL\", \"PROCESS\", \"ALL\", \"CATHODIC PROTECTION\", \"COMMISSIONING\", \n",
    "                   \"DIGITAL\", \"DOCUMENT CONTROL\", \"HVAC\", \"INSTRUMENTATION AND ENVIRONMENT\", \n",
    "                   \"MANUFACTURING\", \"MECHANICAL\", \"METALLURGY\", \"PROCESS\", \"PRODUCTION\", \n",
    "                   \"PROTECTION\", \"QUALITY\", \"SUBCONTRACT ELECTRICAL\", \"TECHNICAL MANAGEMENT\", \n",
    "                   \"STATIC\", \"PROJECT MANAGEMENT\", \"PROJECT CONTROL\"],\n",
    "    \"Month\": [\"Nov-2024\", \"Apr-2024\", \"JAN-2023\", \"SEP-2023\", \"JUL-2023\", \n",
    "              \"FEB-2023\", \"any date in form of mm-yy or mmm-yyyy\", \"current month\", \n",
    "              \"CURRENT MONTH\", \"today\"],\n",
    "    \"Manhour_Manpower_type\": [\"manpower\", \"manhour\", \"man hour\", \"man power\", \n",
    "                              \"manmonths\", \"man months\", \"manmonnth\", \"man month\", \n",
    "                              \"Manhours Plan vs Actuals\", \"Manhour Planned VS Actual\"]\n",
    "}\n",
    " \n",
    "# === INTENT CLASSIFIER COMPONENT ===\n",
    "@Language.factory(\"intent_classifier\")\n",
    "class IntentClassifier:\n",
    "    def __init__(self, nlp, name, model_name=config['nlp']['pipeline'][0]['config']['model_name']):\n",
    "        print(\"Loading tokenizer...\")\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        print(\"Loading model...\")\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(model_name, \n",
    "                                                                     num_labels=len(INTENTS))\n",
    "        print(\"Model loaded successfully!\")\n",
    " \n",
    "    def classify_intent(self, user_input):\n",
    "        print(f\"Classifying intent for input: {user_input}\")\n",
    "        inputs = self.tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_id = torch.argmax(logits).item()\n",
    "        return INTENTS[predicted_class_id]\n",
    " \n",
    "    def __call__(self, doc):\n",
    "        intent = self.classify_intent(doc.text)\n",
    "        doc._.intent = intent\n",
    "        return doc\n",
    " \n",
    "# === ENTITY MATCHER FUNCTION ===\n",
    "def match_entities(text):\n",
    "    matched_entities = []\n",
    "    for entity_type, values in ENTITIES.items():\n",
    "        for value in values:\n",
    "            if value.lower() in text.lower():  # Case insensitive matching\n",
    "                matched_entities.append({\"entity_name\": entity_type, \"value\": value})\n",
    "    return matched_entities\n",
    " \n",
    "# === CUSTOM COMPONENT TO EXTRACT ENTITIES ===\n",
    "@Language.component(\"entity_extractor\")\n",
    "def extract_entities(doc):\n",
    "    matched_entities = match_entities(doc.text)  # Match entities based on user input text\n",
    "    doc._.entities = matched_entities\n",
    "    return doc\n",
    " \n",
    "# === ANNOTATION FUNCTION FOR VISUALIZING ENTITIES ===\n",
    "def annotate_user_input(doc):\n",
    "    \"\"\"\n",
    "    Annotates and highlights entities in the user input using spaCy's displacy.\n",
    "    \"\"\"\n",
    "    # Prepare the visualization data\n",
    "    ents = []\n",
    "    for entity in doc._.entities:\n",
    "        # Find where the entity is in the doc\n",
    "        start = doc.text.find(entity['value'])\n",
    "        end = start + len(entity['value'])\n",
    "        ents.append({\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"label\": entity['entity_name']\n",
    "        })\n",
    "    # Create a custom displacy-friendly structure\n",
    "    displacy_data = {\n",
    "        \"text\": doc.text,\n",
    "        \"ents\": ents,\n",
    "        \"title\": None\n",
    "    }\n",
    " \n",
    "    # Visualize using displacy (spaCy's visualization tool)\n",
    "    displacy.render(displacy_data, style=\"ent\", manual=True)\n",
    " \n",
    "# === MAIN FUNCTION TO PROCESS QUERY ===\n",
    "def process_query(nlp, user_input):\n",
    "    # Process the query with spaCy's pipeline\n",
    "    print(f\"Processing query: {user_input}\")\n",
    "    doc = nlp(user_input)\n",
    " \n",
    "    # Get intent and entities\n",
    "    intent = doc._.intent\n",
    "    entities = doc._.entities\n",
    " \n",
    "    # Annotate the user input to highlight entities\n",
    "    annotate_user_input(doc)\n",
    " \n",
    "    # Return the response with intent and entities\n",
    "    return {\n",
    "        \"User Query\": user_input,\n",
    "        \"Intent\": intent,\n",
    "        \"Entities\": entities\n",
    "    }\n",
    " \n",
    "# === MAIN WORKFLOW SETUP ===\n",
    "def main():\n",
    "    # Load spaCy and add components\n",
    "    print(\"Loading spaCy pipeline...\")\n",
    "    nlp = spacy.blank(\"en\")\n",
    " \n",
    "    # Add custom components to the pipeline\n",
    "    print(\"Adding custom components to pipeline.\")\n",
    "    nlp.add_pipe(\"intent_classifier\", last=True)  # Registering the intent classifier\n",
    "    nlp.add_pipe(\"entity_extractor\", last=True)  # Adding entity extractor after intent classifier\n",
    "    print(\"Components added to pipeline.\")\n",
    " \n",
    "    # Define custom attributes\n",
    "    spacy.tokens.Doc.set_extension(\"intent\", default=None)\n",
    "    spacy.tokens.Doc.set_extension(\"entities\", default=[])\n",
    " \n",
    "    # Loop for dynamic user input\n",
    "    while True:\n",
    "        user_query = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "        if user_query.lower() == 'exit':\n",
    "            break\n",
    "        # Process the query and print the result\n",
    "        response = process_query(nlp, user_query)\n",
    "        print(json.dumps(response, indent=4))\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import yaml\n",
    "import json\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from spacy import displacy\n",
    "from spacy.language import Language\n",
    " \n",
    "# Load the configuration from the YAML file\n",
    "with open('config.yml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    " \n",
    "# INTENTS and ENTITIES from your dataset\n",
    "INTENTS = [\n",
    "    \"Total (Manpower_Manhour) are present in (discipline)\",\n",
    "    \"Total (Manpower_Manhour) are present in (discipline) for (Month)\",\n",
    "    \"Total (Manpower_Manhour) are present in (discipline) from (Month) till (Month)\",\n",
    "    \"Manhours Plan vs Actuals for a (month) and (discipline)\",\n",
    "    \"Difference in Manmonths Plan vs Actuals for a particular month and discipline\",\n",
    "    \"Count of manpower for a particular discipline for a month\"\n",
    "]\n",
    " \n",
    "ENTITIES = {\n",
    "    \"Discipline\": [\"HSE\", \"CIVIL\", \"PROCESS\", \"ALL\", \"CATHODIC PROTECTION\", \"COMMISSIONING\",\n",
    "                   \"DIGITAL\", \"DOCUMENT CONTROL\", \"HVAC\", \"INSTRUMENTATION AND ENVIRONMENT\",\n",
    "                   \"MANUFACTURING\", \"MECHANICAL\", \"METALLURGY\", \"PROCESS\", \"PRODUCTION\",\n",
    "                   \"PROTECTION\", \"QUALITY\", \"SUBCONTRACT ELECTRICAL\", \"TECHNICAL MANAGEMENT\",\n",
    "                   \"STATIC\", \"PROJECT MANAGEMENT\", \"PROJECT CONTROL\"],\n",
    "    \"Month\": [\"Nov-2024\", \"Apr-2024\", \"JAN-2023\", \"SEP-2023\", \"JUL-2023\",\n",
    "              \"FEB-2023\", \"any date in form of mm-yy or mmm-yyyy\", \"current month\",\n",
    "              \"CURRENT MONTH\", \"today\"],\n",
    "    \"Manhour_Manpower_type\": [\"manpower\", \"manhour\", \"man hour\", \"man power\",\n",
    "                              \"manmonths\", \"man months\", \"manmonnth\", \"man month\",\n",
    "                              \"Manhours Plan vs Actuals\", \"Manhour Planned VS Actual\"]\n",
    "}\n",
    " \n",
    "# === INTENT CLASSIFIER COMPONENT ===\n",
    "@Language.factory(\"intent_classifier\")\n",
    "class IntentClassifier:\n",
    "    def __init__(self, nlp, name, model_name=\"roberta-base\"):\n",
    "        print(\"Loading tokenizer...\")\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        print(\"Loading model...\")\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=len(INTENTS))\n",
    "        print(\"Model loaded successfully!\")\n",
    " \n",
    "    def classify_intent(self, user_input):\n",
    "        print(f\"Classifying intent for input: {user_input}\")\n",
    "        inputs = self.tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_id = torch.argmax(logits).item()\n",
    "        return INTENTS[predicted_class_id]\n",
    " \n",
    "    def __call__(self, doc):\n",
    "        intent = self.classify_intent(doc.text)\n",
    "        doc._.intent = intent\n",
    "        return doc\n",
    " \n",
    "# === ENTITY MATCHER FUNCTION ===\n",
    "def match_entities(text):\n",
    "    matched_entities = []\n",
    "    for entity_type, values in ENTITIES.items():\n",
    "        for value in values:\n",
    "            if value.lower() in text.lower():  # Case insensitive matching\n",
    "                matched_entities.append({\"entity_name\": entity_type, \"value\": value})\n",
    "    return matched_entities\n",
    " \n",
    "# === CUSTOM COMPONENT TO EXTRACT ENTITIES ===\n",
    "@Language.component(\"entity_extractor\")\n",
    "def extract_entities(doc):\n",
    "    matched_entities = match_entities(doc.text)  # Match entities based on user input text\n",
    "    doc._.entities = matched_entities\n",
    "    return doc\n",
    " \n",
    "# === ANNOTATION FUNCTION FOR VISUALIZING ENTITIES ===\n",
    "def annotate_user_input(doc):\n",
    "    \"\"\"\n",
    "    Annotates and highlights entities in the user input using spaCy's displacy.\n",
    "    \"\"\"\n",
    "    # Prepare the visualization data\n",
    "    ents = []\n",
    "    for entity in doc._.entities:\n",
    "        # Find where the entity is in the doc\n",
    "        start = doc.text.find(entity['value'])\n",
    "        end = start + len(entity['value'])\n",
    "        ents.append({\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "            \"label\": entity['entity_name']\n",
    "        })\n",
    "    # Create a custom displacy-friendly structure\n",
    "    displacy_data = {\n",
    "        \"text\": doc.text,\n",
    "        \"ents\": ents,\n",
    "        \"title\": None\n",
    "    }\n",
    " \n",
    "    # Visualize using displacy (spaCy's visualization tool)\n",
    "    displacy.render(displacy_data, style=\"ent\", manual=True)\n",
    " \n",
    "# === MAIN FUNCTION TO PROCESS QUERY ===\n",
    "def process_query(nlp, user_input):\n",
    "    # Process the query with spaCy's pipeline\n",
    "    print(f\"Processing query: {user_input}\")\n",
    "    doc = nlp(user_input)\n",
    " \n",
    "    # Get intent and entities\n",
    "    intent = doc._.intent\n",
    "    entities = doc._.entities\n",
    " \n",
    "    # Annotate the user input to highlight entities\n",
    "    annotate_user_input(doc)\n",
    " \n",
    "    # Return the response with intent and entities\n",
    "    return {\n",
    "        \"User Query\": user_input,\n",
    "        \"Intent\": intent,\n",
    "        \"Entities\": entities\n",
    "    }\n",
    " \n",
    "# === MAIN WORKFLOW SETUP ===\n",
    "def main():\n",
    "    # Load spaCy and add components\n",
    "    print(\"Loading spaCy pipeline...\")\n",
    "    nlp = spacy.blank(\"en\")\n",
    " \n",
    "    # Add custom components to the pipeline\n",
    "    print(\"Adding custom components to pipeline.\")\n",
    "    nlp.add_pipe(\"intent_classifier\", last=True)  # Registering the intent classifier\n",
    "    nlp.add_pipe(\"entity_extractor\", last=True)  # Adding entity extractor after intent classifier\n",
    "    print(\"Components added to pipeline.\")\n",
    " \n",
    "    # Define custom attributes\n",
    "    spacy.tokens.Doc.set_extension(\"intent\", default=None)\n",
    "    spacy.tokens.Doc.set_extension(\"entities\", default=[])\n",
    " \n",
    "    # Loop for dynamic user input\n",
    "    while True:\n",
    "        user_query = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "        if user_query.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        # Process the query and print the result\n",
    "        response = process_query(nlp, user_query)\n",
    "        print(json.dumps(response, indent=4))\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "source code not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 71\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m         }\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# === INTENT CLASSIFIER COMPONENT ===\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m \u001b[38;5;129;43m@Language\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mintent_classifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mIntentClassifier\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoading tokenizer...\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Kishan\\chatbot\\venv_chat\\Lib\\site-packages\\spacy\\language.py:514\u001b[0m, in \u001b[0;36mLanguage.factory.<locals>.add_factory\u001b[1;34m(factory_func)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m internal_name \u001b[38;5;129;01min\u001b[39;00m registry\u001b[38;5;241m.\u001b[39mfactories:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;66;03m# We only check for the internal name here â€“ it's okay if it's a\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;66;03m# subclass and the base class has a factory of the same name. We\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;66;03m# also only raise if the function is different to prevent raising\u001b[39;00m\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;66;03m# if module is reloaded.\u001b[39;00m\n\u001b[0;32m    513\u001b[0m     existing_func \u001b[38;5;241m=\u001b[39m registry\u001b[38;5;241m.\u001b[39mfactories\u001b[38;5;241m.\u001b[39mget(internal_name)\n\u001b[1;32m--> 514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_same_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfactory_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexisting_func\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    515\u001b[0m         err \u001b[38;5;241m=\u001b[39m Errors\u001b[38;5;241m.\u001b[39mE004\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    516\u001b[0m             name\u001b[38;5;241m=\u001b[39mname, func\u001b[38;5;241m=\u001b[39mexisting_func, new_func\u001b[38;5;241m=\u001b[39mfactory_func\n\u001b[0;32m    517\u001b[0m         )\n\u001b[0;32m    518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n",
      "File \u001b[1;32mC:\\Kishan\\chatbot\\venv_chat\\Lib\\site-packages\\spacy\\util.py:1143\u001b[0m, in \u001b[0;36mis_same_func\u001b[1;34m(func1, func2)\u001b[0m\n\u001b[0;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m same_name \u001b[38;5;241m=\u001b[39m func1\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m==\u001b[39m func2\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[1;32m-> 1143\u001b[0m same_file \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetfile(func2)\n\u001b[0;32m   1144\u001b[0m same_code \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetsourcelines(func1) \u001b[38;5;241m==\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetsourcelines(func2)\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m same_name \u001b[38;5;129;01mand\u001b[39;00m same_file \u001b[38;5;129;01mand\u001b[39;00m same_code\n",
      "File \u001b[1;32mC:\\Kishan\\chatbot\\venv_chat\\Lib\\site-packages\\torch\\package\\package_importer.py:698\u001b[0m, in \u001b[0;36m_patched_getfile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m _package_imported_modules:\n\u001b[0;32m    697\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _package_imported_modules[\u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m\n\u001b[1;32m--> 698\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_getfile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\Python312\\Lib\\inspect.py:923\u001b[0m, in \u001b[0;36mgetfile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    921\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m\n\u001b[0;32m    922\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 923\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource code not available\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m is a built-in class\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mobject\u001b[39m))\n\u001b[0;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ismethod(\u001b[38;5;28mobject\u001b[39m):\n",
      "\u001b[1;31mOSError\u001b[0m: source code not available"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "import torch\n",
    "\n",
    "import yaml\n",
    "\n",
    "import json\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "from spacy.language import Language\n",
    " \n",
    "# Load the configuration from the YAML file\n",
    "\n",
    "with open('config.yml', 'r') as file:\n",
    "\n",
    "    config = yaml.safe_load(file)\n",
    " \n",
    "# INTENTS and ENTITIES from your dataset\n",
    "\n",
    "INTENTS = config['intents']\n",
    "\n",
    "ENTITIES = config['entities']\n",
    " \n",
    "# === Dataset Class for Training ===\n",
    "\n",
    "class IntentDataset(Dataset):\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "\n",
    "        self.data = []\n",
    "\n",
    "        with open(filepath, 'r') as f:\n",
    "\n",
    "            for line in f:\n",
    "\n",
    "                self.data.append(json.loads(line))\n",
    "\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(config['model_name'])\n",
    " \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        item = self.data[idx]\n",
    "\n",
    "        inputs = self.tokenizer(item['text'], padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "        label = INTENTS.index(item['intent'])\n",
    "\n",
    "        return {\n",
    "\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        }\n",
    " \n",
    "# === INTENT CLASSIFIER COMPONENT ===\n",
    "\n",
    "@Language.factory(\"intent_classifier\")\n",
    "\n",
    "class IntentClassifier:\n",
    "\n",
    "    def __init__(self, nlp, name, model_name=config['model_name']):\n",
    "\n",
    "        print(\"Loading tokenizer...\")\n",
    "\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        print(\"Loading model...\")\n",
    "\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=len(INTENTS))\n",
    "\n",
    "        print(\"Model loaded successfully!\")\n",
    " \n",
    "    def classify_intent(self, user_input):\n",
    "\n",
    "        print(f\"Classifying intent for input: {user_input}\")\n",
    "\n",
    "        inputs = self.tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        outputs = self.model(**inputs)\n",
    "\n",
    "        logits = outputs.logits\n",
    "\n",
    "        predicted_class_id = torch.argmax(logits).item()\n",
    "\n",
    "        return INTENTS[predicted_class_id]\n",
    " \n",
    "    def __call__(self, doc):\n",
    "\n",
    "        intent = self.classify_intent(doc.text)\n",
    "\n",
    "        doc._.intent = intent\n",
    "\n",
    "        return doc\n",
    " \n",
    "# === ENTITY MATCHER FUNCTION ===\n",
    "\n",
    "def match_entities(text):\n",
    "\n",
    "    matched_entities = []\n",
    "\n",
    "    for entity_type, values in ENTITIES.items():\n",
    "\n",
    "        for value in values:\n",
    "\n",
    "            if value.lower() in text.lower():  # Case insensitive matching\n",
    "\n",
    "                matched_entities.append({\"entity_name\": entity_type, \"value\": value})\n",
    "\n",
    "    return matched_entities\n",
    " \n",
    "# === CUSTOM COMPONENT TO EXTRACT ENTITIES ===\n",
    "\n",
    "@Language.component(\"entity_extractor\")\n",
    "\n",
    "def extract_entities(doc):\n",
    "\n",
    "    matched_entities = match_entities(doc.text)  # Match entities based on user input text\n",
    "\n",
    "    doc._.entities = matched_entities\n",
    "\n",
    "    return doc\n",
    " \n",
    "# === ANNOTATION FUNCTION FOR VISUALIZING ENTITIES ===\n",
    "\n",
    "def annotate_user_input(doc):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Annotates and highlights entities in the user input using spaCy's displacy.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare the visualization data\n",
    "\n",
    "    ents = []\n",
    "\n",
    "    for entity in doc._.entities:\n",
    "\n",
    "        # Find where the entity is in the doc\n",
    "\n",
    "        start = doc.text.find(entity['value'])\n",
    "\n",
    "        end = start + len(entity['value'])\n",
    "\n",
    "        ents.append({\n",
    "\n",
    "            \"start\": start,\n",
    "\n",
    "            \"end\": end,\n",
    "\n",
    "            \"label\": entity['entity_name']\n",
    "\n",
    "        })\n",
    "\n",
    "    # Create a custom displacy-friendly structure\n",
    "\n",
    "    displacy_data = {\n",
    "\n",
    "        \"text\": doc.text,\n",
    "\n",
    "        \"ents\": ents,\n",
    "\n",
    "        \"title\": None\n",
    "\n",
    "    }\n",
    " \n",
    "    # Visualize using displacy (spaCy's visualization tool)\n",
    "\n",
    "    displacy.render(displacy_data, style=\"ent\", manual=True)\n",
    " \n",
    "# === TRAINING FUNCTION ===\n",
    "\n",
    "def train_model(train_dataset):\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['training']['batch_size'], shuffle=True)\n",
    "\n",
    "    model = RobertaForSequenceClassification.from_pretrained(config['model_name'], num_labels=len(INTENTS))\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=config['training']['learning_rate'])\n",
    "\n",
    "    num_epochs = config['training']['num_epochs']\n",
    " \n",
    "    # Setup scheduler\n",
    "\n",
    "    num_training_steps = num_epochs * len(train_loader)\n",
    "\n",
    "    scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    " \n",
    "    model.train()\n",
    " \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        for batch in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['label'])\n",
    "\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    " \n",
    "    model.save_pretrained(\"intent_model\")  # Save the trained model\n",
    " \n",
    "# === MAIN FUNCTION TO PROCESS QUERY ===\n",
    "\n",
    "def process_query(nlp, user_input):\n",
    "\n",
    "    # Process the query with spaCy's pipeline\n",
    "\n",
    "    print(f\"Processing query: {user_input}\")\n",
    "\n",
    "    doc = nlp(user_input)\n",
    " \n",
    "    # Get intent and entities\n",
    "\n",
    "    intent = doc._.intent\n",
    "\n",
    "    entities = doc._.entities\n",
    " \n",
    "    # Annotate the user input to highlight entities\n",
    "\n",
    "    annotate_user_input(doc)\n",
    " \n",
    "    # Return the response with intent and entities\n",
    "\n",
    "    return {\n",
    "\n",
    "        \"User Query\": user_input,\n",
    "\n",
    "        \"Intent\": intent,\n",
    "\n",
    "        \"Entities\": entities\n",
    "\n",
    "    }\n",
    " \n",
    "# === MAIN WORKFLOW SETUP ===\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Load spaCy and add components\n",
    "\n",
    "    print(\"Loading spaCy pipeline...\")\n",
    "\n",
    "    nlp = spacy.blank(\"en\")\n",
    " \n",
    "    # Define custom attributes\n",
    "\n",
    "    spacy.tokens.Doc.set_extension(\"intent\", default=None)\n",
    "\n",
    "    spacy.tokens.Doc.set_extension(\"entities\", default=[])\n",
    " \n",
    "    # Add custom components to the pipeline\n",
    "\n",
    "    print(\"Adding custom components to pipeline.\")\n",
    "\n",
    "    nlp.add_pipe(\"intent_classifier\", last=True)  # Registering the intent classifier\n",
    "\n",
    "    nlp.add_pipe(\"entity_extractor\", last=True)  # Adding entity extractor after intent classifier\n",
    "\n",
    "    print(\"Components added to pipeline.\")\n",
    " \n",
    "    # Train the model if data path is provided\n",
    "\n",
    "    if 'data_path' in config['training']:\n",
    "\n",
    "        train_dataset = IntentDataset(config['training']['data_path'])\n",
    "\n",
    "        train_model(train_dataset)\n",
    " \n",
    "    # Loop for dynamic user input\n",
    "\n",
    "    while True:\n",
    "\n",
    "        user_query = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "\n",
    "        if user_query.lower() == 'exit':\n",
    "\n",
    "            break\n",
    "\n",
    "        # Process the query and print the result\n",
    "\n",
    "        response = process_query(nlp, user_query)\n",
    "\n",
    "        print(json.dumps(response, indent=4))\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
